{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from cds_utils import *\n",
    "import librosa ## audio preprocessing\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frame = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(t):\n",
    "    drt, file, n_frame = t\n",
    "    X,Y = [],[]\n",
    "    [x,y],sr = librosa.load(drt+file, sr=16000, mono=False)\n",
    "    audio_length = len(x)\n",
    "    padding = n_frame - audio_length%n_frame\n",
    "    x = np.pad(x,pad_width=(0,padding),mode='constant')\n",
    "    y = np.pad(y,pad_width=(0,padding),mode='constant')\n",
    "    for frame in range(audio_length//n_frame+1):\n",
    "        X.append(x[frame*n_frame:(frame+1)*n_frame])\n",
    "        Y.append(y[frame*n_frame:(frame+1)*n_frame])\n",
    "    return X,Y\n",
    "\n",
    "def load_datas(n_frame,is_test = False):\n",
    "    X,y = [],[]\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    \n",
    "    print(\"load Train Datas\")\n",
    "    args = [] ## [..(drt,file,n_frame)..]\n",
    "    \n",
    "    for drt in ['./datasets/TrainData/Alexis/','./datasets/TrainData/vietnam/','./datasets/TrainData/Childer/',\n",
    "               './datasets/TrainData/CMU/','./datasets/TrainData/saarbrucken/']:\n",
    "        for file in os.listdir(drt):\n",
    "            if 'wav' in file:\n",
    "                args.append((drt,file,n_frame))\n",
    "    \n",
    "    if is_test:\n",
    "        args = args[:50]\n",
    "    \n",
    "    tmp = pool.map(process,args) ## [..[X,Y]..] X = [...[20000][20000][20000]...]\n",
    "    for _X,_Y in tmp: ## _X : [...[20000]...]\n",
    "        X += _X\n",
    "        y += _Y\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    train_X,val_X,train_y,val_y = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "    return train_X,train_y,val_X,val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Train Datas\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, val_X, val_y = load_datas(n_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parallel_train(t):\n",
    "    i,n_frame = t\n",
    "    np.save('./datasets/TrainData/trainX_%d/%d.npy'%(n_frame,i),train_X[i])\n",
    "    np.save('./datasets/TrainData/trainy_%d/%d.npy'%(n_frame,i),train_y[i])\n",
    "    return 0\n",
    "\n",
    "def save_parallel_val(t):\n",
    "    i,n_frame = t\n",
    "    np.save('./datasets/TrainData/valX_%d/%d.npy'%(n_frame,i),val_X[i])\n",
    "    np.save('./datasets/TrainData/valy_%d/%d.npy'%(n_frame,i),val_y[i])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 ms, sys: 0 ns, total: 1.91 ms\n",
      "Wall time: 1.03 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.makedirs('./datasets/TrainData/trainX_%d/'%n_frame,exist_ok=True)\n",
    "os.makedirs('./datasets/TrainData/valX_%d/'%n_frame,exist_ok=True)\n",
    "os.makedirs('./datasets/TrainData/trainy_%d/'%n_frame,exist_ok=True)\n",
    "os.makedirs('./datasets/TrainData/valy_%d/'%n_frame,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 138 ms, sys: 2.03 s, total: 2.17 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "pool.map(save_parallel_train,zip(range(train_X.shape[0]),[n_frame]*train_X.shape[0]))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.3 ms, sys: 2.07 s, total: 2.13 s\n",
      "Wall time: 6.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "pool.map(save_parallel_val,zip(range(val_X.shape[0]),[n_frame]*val_X.shape[0]))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
