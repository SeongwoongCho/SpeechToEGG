{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from scipy.signal import butter,filtfilt,find_peaks,find_peaks_cwt,medfilt,savgol_filter\n",
    "from utils import butter_lowpass_filter, butter_highpass_filter, smooth, positions2onehot, normalize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from test_metric_utils import *\n",
    "from model_unet import Unet\n",
    "from model_AAE import FCAE\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model,Speech,n_frame = 192,window_step = 32):\n",
    "    assert n_frame%window_step ==0\n",
    "    \n",
    "    model.eval()\n",
    "    Speech = np.expand_dims(Speech,axis=-1)\n",
    "    EGG_pred = np.zeros_like(Speech)\n",
    "    ratio = np.zeros_like(Speech)\n",
    "    frame = 0\n",
    "    \n",
    "    while frame*window_step + n_frame <= len(Speech):\n",
    "        tmp = Speech[frame*window_step:frame*window_step+n_frame]\n",
    "        tmp = torch.Tensor([normalize(tmp)]).cuda() ## preprocessing\n",
    "        \n",
    "        result = model(tmp).cpu().detach().numpy()[0]\n",
    "        EGG_pred[frame*window_step:frame*window_step+n_frame] += result ## postprocessing\n",
    "        ratio[frame*window_step:frame*window_step+n_frame] +=1\n",
    "        frame +=1\n",
    "    \n",
    "    for i in range(len(EGG_pred)):\n",
    "        if ratio[i]!=0:\n",
    "            EGG_pred[i] = EGG_pred[i]/ratio[i]\n",
    "    return EGG_pred[:n_frame + frame*window_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCAE(\n",
       "  (encoder): DataParallel(\n",
       "    (module): FCEncoder(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Linear(in_features=192, out_features=175, bias=True)\n",
       "        (1): Linear(in_features=175, out_features=125, bias=True)\n",
       "        (2): Linear(in_features=125, out_features=100, bias=True)\n",
       "      )\n",
       "      (batches): ModuleList(\n",
       "        (0): BatchNorm1d(175, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): DataParallel(\n",
       "    (module): FCDecoder(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Linear(in_features=100, out_features=125, bias=True)\n",
       "        (1): Linear(in_features=125, out_features=175, bias=True)\n",
       "        (2): Linear(in_features=175, out_features=192, bias=True)\n",
       "      )\n",
       "      (batches): ModuleList(\n",
       "        (0): BatchNorm1d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(175, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAE_cos = FCAE()\n",
    "AAE_cos.encoder = nn.DataParallel(AAE_cos.encoder)\n",
    "AAE_cos.decoder = nn.DataParallel(AAE_cos.decoder)\n",
    "AAE_cos.encoder.load_state_dict(torch.load(\"./models/AAI/STZ-cosloss.pth\"))\n",
    "AAE_cos.decoder.load_state_dict(torch.load(\"./models/AAI/ZTE-cosloss.pth\"))\n",
    "AAE_cos.cuda()\n",
    "# unet_model = nn.DataParallel(model)\n",
    "# unet_model.load_state_dict(torch.load(\"./models/Unet/best_val.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Unet(\n",
       "    (encoder): ModuleList(\n",
       "      (0): Conv1d(1, 10, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (1): Conv1d(10, 20, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (2): Conv1d(20, 30, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (3): Conv1d(30, 40, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    )\n",
       "    (decoder): ModuleList(\n",
       "      (0): Conv1d(80, 40, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): Conv1d(70, 30, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (2): Conv1d(50, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (3): Conv1d(30, 10, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    )\n",
       "    (ebatch): ModuleList(\n",
       "      (0): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dbatch): ModuleList(\n",
       "      (0): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (middle): Sequential(\n",
       "      (0): Conv1d(40, 40, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): Conv1d(11, 1, kernel_size=(1,), stride=(1,))\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_cos = Unet(4,10)\n",
    "unet_cos = nn.DataParallel(unet_cos)\n",
    "unet_cos.load_state_dict(torch.load(\"./models/Unet/best-cosloss.pth\"))\n",
    "unet_cos.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CMU Data\n",
    "## Normal person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(EGG):\n",
    "#     EGG = normalize(EGG)\n",
    "    peak_range = (7,15)\n",
    "    DEGG = np.gradient(EGG,edge_order = 2)\n",
    "    DEGG = medfilt(DEGG, 3)\n",
    "    DEGG_low,EGG_low = DEGG.copy(),EGG.copy()\n",
    "    \n",
    "    DEGG_low[DEGG_low>0] =0\n",
    "    EGG_low[EGG_low>0] =0\n",
    "    \n",
    "    DEGG_low = find_peaks_cwt(-DEGG_low,np.arange(*peak_range))\n",
    "    EGG_low = find_peaks_cwt(-EGG_low,np.arange(*peak_range))\n",
    "\n",
    "    DEGG_high = []\n",
    "    for i in range(len(DEGG_low)-1):\n",
    "        DEGG_high.append(DEGG_low[i] + np.argmax(DEGG[DEGG_low[i]:DEGG_low[i+1]]))\n",
    "\n",
    "    EGG_high = []\n",
    "    for i in range(len(EGG_low)-1):\n",
    "        EGG_high.append(EGG_low[i] + np.argmax(EGG[EGG_low[i]:EGG_low[i+1]]))\n",
    "    \n",
    "    DEGG_high, EGG_high = np.array(DEGG_high),np.array(EGG_high)\n",
    "    \n",
    "    return DEGG_high/16000,DEGG_low/16000,EGG_high/16000,EGG_low/16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d4504cf7c04194a1f5bacda67cef84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1614), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scipy/signal/_peak_finding.py:1113: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  snr = abs(cwt[line[0][0], line[1][0]] / noises[line[1][0]])\n"
     ]
    }
   ],
   "source": [
    "window_step = 64\n",
    "directory = './datasets/TestData/CMU/'\n",
    "filelist = os.listdir(directory)\n",
    "\n",
    "DEGG_high_metrics_final = {'IDR':0, 'MR' : 0, 'FAR':0 ,'IDA':0}\n",
    "DEGG_low_metrics_final = {'IDR':0, 'MR' : 0, 'FAR':0 ,'IDA':0}\n",
    "EGG_high_metrics_final = {'IDR':0, 'MR' : 0, 'FAR':0 ,'IDA':0}\n",
    "EGG_low_metrics_final = {'IDR':0, 'MR' : 0, 'FAR':0 ,'IDA':0}\n",
    "# filelist = filelist[:]\n",
    "for file in tqdm_notebook(filelist):\n",
    "    [Speech,EGG_true],sr = librosa.load(directory + file,sr=16000,mono=False)\n",
    "    Speech = butter_lowpass_filter(Speech,2500,16000)\n",
    "    itvs = librosa.effects.split(Speech,frame_length = int(192*0.75), hop_length = int(192*0.25),top_db = 10)\n",
    "    \n",
    "    S = []\n",
    "    E = []\n",
    "    for st,ed in itvs:\n",
    "        S += list(Speech[st:ed])\n",
    "        E += list(EGG_true[st:ed])\n",
    "    \n",
    "    Speech = np.array(S)\n",
    "    EGG_true = np.array(E)\n",
    "    \n",
    "    EGG_pred = inference(unet_cos,Speech,n_frame = 192,window_step = window_step)\n",
    "    EGG_pred = np.squeeze(EGG_pred,axis=-1)\n",
    "    EGG_pred = smooth(EGG_pred, 49)\n",
    "    \n",
    "    l = min(len(EGG_pred),len(EGG_true))\n",
    "    EGG_true =EGG_true[:l]\n",
    "    EGG_pred =EGG_pred[:l]\n",
    "    \n",
    "    DEGG_high_true,DEGG_low_true, EGG_high_true, EGG_low_true = get_points(EGG_true)\n",
    "    DEGG_high_pred,DEGG_low_pred, EGG_high_pred, EGG_low_pred = get_points(EGG_pred)\n",
    "    \n",
    "    DEGG_high_metrics = corrected_naylor_metrics(DEGG_high_true, DEGG_high_pred) ##GOI\n",
    "    DEGG_low_metrics = corrected_naylor_metrics(DEGG_low_true, DEGG_low_pred) ##GCI\n",
    "    EGG_high_metrics = corrected_naylor_metrics(EGG_high_true, EGG_high_pred)\n",
    "    EGG_low_metrics = corrected_naylor_metrics(EGG_low_true, EGG_low_pred)\n",
    "    \n",
    "    DEGG_high_metrics_final['IDR'] += DEGG_high_metrics[\"identification_rate\"]/len(filelist)\n",
    "    DEGG_high_metrics_final['MR'] += DEGG_high_metrics[\"miss_rate\"]/len(filelist)\n",
    "    DEGG_high_metrics_final['FAR'] += DEGG_high_metrics[\"false_alarm_rate\"]/len(filelist)\n",
    "    DEGG_high_metrics_final['IDA'] += DEGG_high_metrics[\"identification_accuracy\"]/len(filelist)\n",
    "\n",
    "    DEGG_low_metrics_final['IDR'] += DEGG_low_metrics[\"identification_rate\"]/len(filelist)\n",
    "    DEGG_low_metrics_final['MR'] += DEGG_low_metrics[\"miss_rate\"]/len(filelist)\n",
    "    DEGG_low_metrics_final['FAR'] += DEGG_low_metrics[\"false_alarm_rate\"]/len(filelist)\n",
    "    DEGG_low_metrics_final['IDA'] += DEGG_low_metrics[\"identification_accuracy\"]/len(filelist)\n",
    "\n",
    "    EGG_high_metrics_final['IDR'] += EGG_high_metrics[\"identification_rate\"]/len(filelist)\n",
    "    EGG_high_metrics_final['MR'] += EGG_high_metrics[\"miss_rate\"]/len(filelist)\n",
    "    EGG_high_metrics_final['FAR'] += EGG_high_metrics[\"false_alarm_rate\"]/len(filelist)\n",
    "    EGG_high_metrics_final['IDA'] += EGG_high_metrics[\"identification_accuracy\"]/len(filelist)\n",
    "\n",
    "    EGG_low_metrics_final['IDR'] += EGG_low_metrics[\"identification_rate\"]/len(filelist)\n",
    "    EGG_low_metrics_final['MR'] += EGG_low_metrics[\"miss_rate\"]/len(filelist)\n",
    "    EGG_low_metrics_final['FAR'] += EGG_low_metrics[\"false_alarm_rate\"]/len(filelist)\n",
    "    EGG_low_metrics_final['IDA'] += EGG_low_metrics[\"identification_accuracy\"]/len(filelist)\n",
    "print(\"=========DEGG_high(GOI) detection========\")\n",
    "print(\"IDR : %.2f MR : %.2f FAR : %.2f IDA : %.2f ms\"\n",
    "      %(DEGG_high_metrics_final['IDR']*100,DEGG_high_metrics_final['MR']*100,DEGG_high_metrics_final['FAR']*100,DEGG_high_metrics_final['IDA']*1000))\n",
    "\n",
    "print(\"=========DEGG_low(GCI) detection========\")\n",
    "print(\"IDR : %.2f MR : %.2f FAR : %.2f IDA : %.2f ms\"\n",
    "      %(DEGG_low_metrics_final['IDR']*100,DEGG_low_metrics_final['MR']*100,DEGG_low_metrics_final['FAR']*100,DEGG_low_metrics_final['IDA']*1000))\n",
    "\n",
    "print(\"=========EGG_high detection========\")\n",
    "print(\"IDR : %.2f MR : %.2f FAR : %.2f IDA : %.2f ms\"\n",
    "      %(EGG_high_metrics_final['IDR']*100,EGG_high_metrics_final['MR']*100,EGG_high_metrics_final['FAR']*100,EGG_high_metrics_final['IDA']*1000))\n",
    "\n",
    "print(\"=========EGG_low detection========\")\n",
    "print(\"IDR : %.2f MR : %.2f FAR : %.2f IDA : %.2f ms\"\n",
    "      %(EGG_low_metrics_final['IDR']*100,EGG_low_metrics_final['MR']*100,EGG_low_metrics_final['FAR']*100,EGG_low_metrics_final['IDA']*1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST saarbrucken Data\n",
    "## Pathologic person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
